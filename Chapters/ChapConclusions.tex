%*****************************************
\chapter{Conclusions}\label{ch:conclusions}
%*****************************************

\section*{Overall Conclusions}\label{sec:overall-conclusions}

This thesis sought to be an initial step in using deep linguistic knowledge to develop a \ac{CAPT} for Brazilian-accented English. Back in 2013, when this project started, one of the core challenges of researches in \ac{CAPT} was that \ac{ASR} systems were not precise enough in terms of phone recognition \cite{Witt2012}, what posed serious problems for \ac{CAPT} since phones are necessarily the basic signal that pronunciation training systems must rely on. Our first hypothesis was that by using as much phonetic knowledge in all stages of the pipeline for a \ac{CAPT}, we would be able to obtain more robust results in  phone recognition, thus improving the quality of pronunciation assessment and pushing the state of the art forward. The plan was to focus especially in the \ac{ASR}, not only by training acoustic models that would be representantive of the phones in Brazilian-accented English, but also enriching the pronunciation dictionary with relevant mispronunciations. 

However as time went by, this initial project has proven to be undoable for a Master's project not only with respect to the time or resources available, but also in terms of complexity. The project was too ambitious and results in \ac{ASR} are often exploratory. The truth is that \ac{ASR} is a huge interdisciplinary area which, at present time, is still not solved. Even large IT companies which have at their disposal the best algorithms, computer power and speech corpora in the world still report \ac{WER} results around 12\% for conversational speech  \cite{Huang2014}. This \ac{WER} result means that a short sentence with 4 words is fully recognized only 59\% of the times.

Due to the complexity of the task and the scarcity of resources \cite{Neto2011}, replicating for Brazilian-accented English other methods that were successfully applied to other languages was unfeasiable. There was not enough data for training acoustic models, specialized language models or dictionaries which included entries with common mispronunciations. Our approach was then more conservative and focused on resources for Natural Language Processing and Pronunciation Evaluation that can help a future development of a \ac{CAPT} system for Brazilian-accented English. More especifically, our focus was on tasks for text-to-speech, spelling correction, corpus building and automatic pronunciation assessment -- our approach in all of them was to include enrich the models with phonetic knowledge in order to increase their performance. As it was shown in the papers presented, improvements were made and we were able to push the state of the art one step further in several occasions\footnote{The resources are available at the project website \emph{(http://nilc.icmc.usp.br/listener)}. Due to copyright reasons, the corpora used for training the acoustic models cannot be made available.}:

\paragraph*{Text-to-speech}
  \begin{enumerate}
    \item The hybrid approach that we proposed to text-to-speech, which makes use of manual rules and machine learning, has proven to be quite efficient. Not only the results are in the state of the art, but also the approach is the quite flexible and scalable. The architecture of Aeiouado allows one to easily work on improving the accuracy/recall of the system by embedding more phonetic knowledge through reviewing the transcription rules, or by enlarging the test set, for instance, providing new examples for training.
    \item Supra-segmental information has shown to be useful feature for the machine learning classifier. As can be seen by the results in the paper \cite{Mendonca2014}, the model was able to learn the difference between pretonic/tonic vs. postonic vowels. The f1-measure for [\ipa{U, @, I}] was 1.00, 0.99 and 0.97, respectively.
    \item Part-of-speech were able to help the model to learn the difference in heterophonic homograph pairs to a certain extent. The G2P was able to correctly infer heterophonic homograph pairs that were not seen during training, such as "ab[\textipa{o}]rto" (abortion) and "ab[\textipa{O}]rto" (I abort), or "ap[\textipa{o}]sto" (apposition) and "ap[\textipa{O}]sto" (I bet).
  \end{enumerate}

\paragraph*{Spelling correction}
  \begin{enumerate}
    \setcounter{enumi}{4}
    \item The hypothesis that phonetic knowledge could be used to improve the results of the speller due to phonologically-motivated errors was correct. The baseline system which had no phonetic module was able to correct 48.2\% of non-contextual phonologically motivated errors in the corpus. In comparison, one of the methods which considered the distance between phonetic transcriptions was  able to achieve 87.1\% accuracy in the same task.
    \item Our assumption about the types of mispellings also held. Around 18\% of the mispelling errors in user-generated content were phonologically-motivated. We compiled, annotated and released to the public a corpus for spelling correction tasks with 38,128 tokens, 4,083 of which containing mispelled words. It is worth noticing that there were no open corpora to evaluate this task in Portuguese, prior to this work.
  \end{enumerate}

\paragraph*{Corpus building}
  \begin{enumerate}
    \setcounter{enumi}{5}
    \item Our hypothesis that greedy algorithms would be a suitable way for extracting phonetic-ally-rich sentences from a corpora was supported. The results showed that the greedy strategy was capable of extracting sentences in a much more uniform way, while comparing to a random selection. For instance, the method was able to extract 854 new distinct triphones for a sample of 250 sentences, with almost twice the type/token ration of the random sample -- 0.61 vs. 0.32, respectively.
  \end{enumerate}

\paragraph*{Automatic pronunciation assessment}
  \begin{enumerate}
    \setcounter{enumi}{6}
    \item The multipronunciation dictionary with manually rules has shown to be a reliable source of phonetic information for pronunciation assessment.
    \item Our hypothesis about the acoustic models could be verified as the performance of the prototype was severely affected by noise in the data.
    \item Context-free grammars were successfully adapted to perform forced-alignment.
    \item The additional pronunciation variants did not harm the performance, as expected.
  \end{enumerate}

\section*{Limitations and Further Work}\label{sec:limitations-further}

Although the research has reached its partial aims in building tools and resources for Natural Language Processing and Pronunciation Evaluation, there were still some unavoidable limitations. We conclude this thesis by discussing the limitations and providing ideas for future research.

\paragraph*{Text-to-speech}
Despite using part-of-speech information to capture the difference in heterophone homograph pairs, this did provide all context that mid vowels need. The worst results were related to the transcription of mid vowels [\ipa{E, e, O, o}]. Particularly, the model was very confused about mid-low ones, [\ipa{E}] showed an F1-score 0.66 and [\ipa{O}] of 0.71. In future studies, it might be interesting to see if any other features could be used to improve the model performance in distinguishing the difference between [\ipa{e,o}] and [\ipa{E,O}], respectively. Exception dictionaries or post-processing rules can be used to increase the accuracy \cite{Shulby2013}. It can also be interesting to see if the errors are somehow related to vowel harmony (p[\ipa{E}]r[\ipa{E}]r[\ipa{E}]ca) or to suffixation (p[\ipa{E}] > p[\ipa{E}]zinho). One could also check whether training the models with more data would suffice to solve the issues reported here.

\paragraph*{Spelling correction}
The architecture of the GPM-ML speller was quite complex and used features from many different sources (phonetic transcription, language model, Keyboard model and string edit distance), which, theoretically, would provide enough information for the classifier to learn the mispelling patterns. Although the speller presented results from the state of the art, our initial expectation was that it would have even a better performance, especially with respect to general typos, e.g. ``fala'' (speech) > ``fdala'' (speecxh). These cases would generate very low probabilities in the language model, and the keyboard model would be able to indicate that ``f'' and ``d'' are adjacent keys, so the pattern for identifying these errors should be available for the classifier. However, in comparison to the other method, which had neither a language model nor a keyboard model, there was just a 2\% improvement for general typos. It 
might be interesting, in a future work, to investigate why the impact was rather small. Further research could also focus in trying to improve the language model in order to tackle contextual errors. We assumed that, by using probabilities from a language model estimated from news texts, the classifier would learn the difference between sequences of words with correct spelling and those with errors. However, this was true just for contextual diacrictic errors, not for those contextual errors which were phonologically-motivated. The speller was able to automatically fix real-errors with diacrictics, such as ``ela \emph{e} inteligente'' (lit. she and smart) -> ``ela \emph{Ã©} inteligente'' (she is smart); but it was not able to generalize this to other contexts, such as ``lojas do \emph{seguimento}'' (lit. stores of the following) -> *``lojas do \emph{segmento}''(related business, lit. stores of the segment). It is worth noticing that the language model was estimated over a subset of the Corpus Brasileiro (circa 10 million tokens) and it was not filtered before estimating the probabilities. The speller cuold benefit from corpora with less noise.  More powerful methods of machine learning could also be employed to improve the performance.

\paragraph*{Corpus building}
The method we proposed was able to extract sentences with much more uniform triphones as the type/token ration confirms, however, since the method basically favours rare triphones in each iteration, some of the sentences that were extracted had very awkward reading or uncommon words. Considering that the method is useful especially for preparing prompts to build speech recognition corpora, this poses a problem: if the prompt is not understood, the voice donor might hesitate or read the sentence in a way that is not natural. In the future, it might be interesting to develop some sort of filter during each iteration of the algorithm, to remove these sentences with problems, while keeping the triphone balance. In addition to this, one of the main reasons for building the algorithm was to test whether phonetically-rich corpora would be more benefitial to phone recognition than phonetically-balanced corpora. Our hypothesis was that  since the phones are sampled more equally, acoustic models would be able to better estimate the phone properties, thus leading to improvements in tasks which demand robust phone models, such as phone recognition or forced-alignment. But due to time constraints this hypothesis could not be tested. If supported, this may be quite interesting for application which require very accurate phone models, such as \ac{CAPT} or speaker verification.

\paragraph*{Automatic pronunciation assessment}
The prototype has achieved a very high accuracy in the Induced corpus, with a true positive rate of 0.90, meaning that each mispronunciation pattern that really occurred in the corpus were correctly identified 90\% of the time. However, the Induced corpus is speaker-dependent and contains clean speech data. While evaluating the Listener Corpus, which has a considerable amount of the noise, the system performed much worse: the true positive rates obtained for both test sets from this corpus (0.57 and 0.31) are unseemly for developing real applications in automatic pronunciation assessment. As a future work, it might be interesting to evaluate the prototype with data that is not as clean the Induced corpus and that is, also, not as noisy as the test sets from the Listener Corpus. For instance, a corpus recorded from mobile phones would fill both these criteria. It could also be interesting to enlarge the training set, in order to build more robust acoustic models.

%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
